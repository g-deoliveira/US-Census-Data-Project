{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Model Selection and Training\n",
    "Guilherme de Oliveira <br>\n",
    "9/9/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Chapter 2 we will work on the classification model of the US Census data that was analyzed in Chapter 1. My biggest interest in modelling will be dealing with the class imbalance of the target variable. In particular, I am interested in the following aspects:\n",
    "<ul>\n",
    "<li> How best to assess the accuracy of the classifier. It is unlikely that accuracy will suffice, because of the [accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox).\n",
    "<li> What are some approaches that we can use to deal with the class imbalance? Examples include oversampling, undersampling, incorporating clustering algorithms, etc...\n",
    "</ul>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "# This is a work in progress. Stay tuned for more...\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing function - developped in Chapter 1\n",
    "the_columns  = [('age', 'continuous'), \n",
    "                ('class_of_worker', 'nominal'), \n",
    "                ('detailed_industry_code', 'nominal'), \n",
    "                ('detailed_occupation_code', 'nominal'), \n",
    "                ('education', 'nominal'), \n",
    "                ('wage_per_hour', 'continuous'), \n",
    "                ('enrolled_in_edu_last_week', 'nominal'),\n",
    "                ('marital_status', 'nominal'),\n",
    "                ('major_industry_code', 'nominal'),\n",
    "                ('major_occupation_code', 'nominal'),\n",
    "                ('race', 'nominal'),\n",
    "                ('hispanic_origin', 'nominal'),\n",
    "                ('sex', 'binary'), # binary column with values Male/Female\n",
    "                ('member_of_labor_union', 'nominal'), \n",
    "                ('reason_for_unemployment', 'nominal'),\n",
    "                ('full_or_part_time_employment_stat', 'nominal'),\n",
    "                ('capital_gains', 'continuous'),\n",
    "                ('capital_losses', 'continuous'),\n",
    "                ('dividends', 'continuous'),\n",
    "                ('tax_filer', 'nominal'),\n",
    "                ('region_of_previous_residence', 'nominal'),\n",
    "                ('state_of_previous_residence', 'nominal'),\n",
    "                ('detailed_household_family_stat', 'nominal'),\n",
    "                ('detailed_household_summary', 'nominal'),\n",
    "                ('instance_weight', 'IGNORE'), # as per instructions, to be dropped\n",
    "                ('migration_code_change_in_msa', 'nominal'),\n",
    "                ('migration_code_change_in_reg', 'nominal'),\n",
    "                ('migration_code_move_within_reg', 'nominal'),\n",
    "                ('live_in_this_house_1_yr_ago', 'nominal'),\n",
    "                ('migration_prev_res_in_sunbelt', 'nominal'),\n",
    "                ('num_persons_worked_for_employer', 'continuous'),\n",
    "                ('family_members_under_18', 'nominal'),\n",
    "                ('cob_father', 'nominal'),\n",
    "                ('cob_mother', 'nominal'),\n",
    "                ('cob_self', 'nominal'),\n",
    "                ('citizenship', 'nominal'),\n",
    "                ('own_business_or_self_employed', 'nominal'),\n",
    "                ('fill_in_questionnaire_for_veterans_admin', 'nominal'),\n",
    "                ('veterans_benefits', 'nominal'),\n",
    "                ('weeks_worked_in_year', 'nominal'),\n",
    "                ('year', 'nominal'), \n",
    "                ('savings','target')] # binary TARGET variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessData(file_name):\n",
    "    # the_columns stores tuples of (column_name and tag for continuous/nominal/binary/target)\n",
    "    \n",
    "    raw_data = pd.read_csv(file_name, names=[c[0] for c in the_columns], index_col=False)\n",
    "    original_shape = raw_data.shape\n",
    "    \n",
    "    raw_data.drop('instance_weight', axis=1, inplace=True)\n",
    "    the_columns.remove(('instance_weight', 'IGNORE'))\n",
    "    \n",
    "    # find the duplicate rows, keep the first one\n",
    "    duplicate_rows = raw_data.duplicated(keep='first')\n",
    "    \n",
    "    print 'number of duplicates = {:d}'.format(duplicate_rows.sum())\n",
    "    raw_data = raw_data.drop_duplicates(keep='first')\n",
    "    new_shape =  raw_data.shape\n",
    "    print 'number of duplicates removed = {:d}'.format(original_shape[0] - new_shape[0])\n",
    "    print 'original shape = {:d}, {:d}'.format(original_shape[0], original_shape[1])\n",
    "    print 'new shape = {:d}, {:d}'.format(raw_data.shape[0], raw_data.shape[1])\n",
    "    \n",
    "    # convert nominal columns (object dtype) to integer type\n",
    "    data = pd.DataFrame(raw_data.select_dtypes(include=['object']))\n",
    "    object_columns = data.columns\n",
    "    \n",
    "    for column in object_columns:\n",
    "        unique_values = data[column].unique()\n",
    "        dictionary = {key:idx for idx,key in enumerate(unique_values)}\n",
    "        data[column] = data[column].apply(lambda x : dictionary[x])\n",
    "    \n",
    "    # add nominal columns that were already in integer format \n",
    "    nominal_integer_columns = [c[0] for c in the_columns \n",
    "                               if c[1] == 'nominal' and c[0] not in data.columns]\n",
    "    data[nominal_integer_columns] = raw_data[nominal_integer_columns]\n",
    "    \n",
    "    # convert 'sex', and 'savings' columns to binary; add year column\n",
    "    data['savings'] = raw_data['savings'].map(lambda x: \n",
    "                                              1 if str(x).strip() == '50000+.' else 0)\n",
    "    data['sex'] = raw_data['sex'].map(lambda x: \n",
    "                                      1 if str(x).strip() == 'Male' else 0)\n",
    "    data['year'] = raw_data['year']\n",
    "    \n",
    "    # add continuous columns\n",
    "    continuous_columns = [c[0] for c in the_columns if c[1] == 'continuous']\n",
    "    data[continuous_columns] = raw_data[continuous_columns]\n",
    "    \n",
    "    # verify that we aren't missing any columns\n",
    "    assert set(data.columns) == (set(raw_data.columns))\n",
    "    \n",
    "    text = 'The final processed data has {:,d} rows and {:d} columns.\\n'\n",
    "    print text.format(data.shape[0], data.shape[1])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = preprocessData('us_census_full/census_income_learn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoded Data\n",
    "This will be required for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_nominal_columns():\n",
    "    nominal_columns = [c[0] for c in the_columns if c[1] == 'nominal']\n",
    "    \n",
    "    dummy_columns = [pd.get_dummies(data[col], prefix=col, prefix_sep='.') \n",
    "                     for col in nominal_columns]\n",
    "    \n",
    "    one_hot_encoded_data = pd.concat(dummy_columns, axis=1)\n",
    "    print '\\nThere were {:d} nominal columns to be converted.'.format(len(nominal_columns))\n",
    "    print 'The number of one-hot-encoded columns is {:d}.\\n'.format(data.shape[1])\n",
    "    \n",
    "    # check size\n",
    "    count_distinct_values = 0\n",
    "    for column in nominal_columns:\n",
    "        count_distinct_values += len(data[column].unique())\n",
    "    \n",
    "    assert count_distinct_values == one_hot_encoded_data.shape[1], \\\n",
    "        \"mismatch between number of dummy columns and unique values\"\n",
    "    \n",
    "    return one_hot_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohe_data = one_hot_encode_nominal_columns()\n",
    "\n",
    "# add target (savings)\n",
    "ohe_data['savings'] = data['savings']\n",
    "\n",
    "# scale and add continuous columns\n",
    "min_max_scaler = MinMaxScaler()\n",
    "continuous_cols = [c[0] for c in the_columns if c[1] == 'continuous']\n",
    "ohe_data[continuous_cols] = pd.DataFrame(min_max_scaler.fit_transform(\n",
    "        data[continuous_cols]), columns=continuous_cols, index = data.index)\n",
    "\n",
    "print 'The final shape is: {:,d} x {:d}.'.format(ohe_data.shape[0], ohe_data.shape[1])\n",
    "mx = ohe_data.max().max()\n",
    "mn = ohe_data.min().min()\n",
    "print 'To verify scaling: max = {:.2f}, min={:.2f}\\n'.format(mx, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_data_sets(X, y):\n",
    "    # obtain training and test set for cross-validation\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    print 'size of training data: {:7d}, {:3d}'.format(X_train.shape[0], X_train.shape[1])\n",
    "    print 'size of test data:     {:7d}, {:3d}'.format(X_test.shape[0], X_test.shape[1])\n",
    "    print\n",
    "    ytr0, ytr1 = (y_train == 0).sum(), (y_train == 1).sum()\n",
    "    yte0, yte1 = (y_test == 0).sum(), (y_test == 1).sum()\n",
    "    print 'y_train==0: {:6d},  y_train==1: {:4d},  balance: {:.4f}'.format(\n",
    "        ytr0, ytr1, float(ytr0)/(ytr0+ytr1))\n",
    "    print 'y_test==0:  {:6d},  y_test==1:  {:4d},  balance: {:.4f}'.format(\n",
    "        yte0, yte1, float(yte0)/(yte0+yte1))\n",
    "    print '\\nThe training and test set appear to have the same degree of class imbalance.\\n'\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    header = '\\t          prediction 0    prediction 1'\n",
    "    row0 =   '\\tclass 0 {:11,d} {:14,d}'\n",
    "    row1 =   '\\tclass 1 {:11,d} {:14,d}'\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print header\n",
    "    print row0.format(cm[0,0], cm[0,1])\n",
    "    print row1.format(cm[1,0], cm[1,1])\n",
    "    tp, fn = float(cm[0,0]), float(cm[0,1])\n",
    "    fp, tn = float(cm[1,0]), float(cm[1,1])\n",
    "#     print 'precision = {:.4f},  {:.4f}'.format(tp/(tp+fp), tn/(tn+fn))\n",
    "#     print 'recall =    {:.4f},  {:.4f}'.format(tp/(tp+fn), tn/(tn+fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_grid_search(classifier, parameters, score=None, print_grid_scores=False, verbose=0):\n",
    "    \n",
    "    rf_clf = GridSearchCV(classifier, \n",
    "                          parameters, \n",
    "                          scoring=score,\n",
    "                          verbose=verbose)\n",
    "    \n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    \n",
    "    print 'Best parameters on training set:'\n",
    "    print rf_clf.best_params_\n",
    "    print '\\nBest score = {:.4f}'.format(rf_clf.best_score_)\n",
    "    if print_grid_scores:\n",
    "        print '\\nGrid scores on training set:\\n'\n",
    "        for params, mean_score, scores in rf_clf.grid_scores_:\n",
    "            print(\"%0.4f (+/-%0.04f) for %r\"\n",
    "                  % (mean_score, scores.std() * 2, params))\n",
    "    print\n",
    "    print 'confusion matrix'\n",
    "    print_confusion_matrix(y_test, y_pred)\n",
    "    print '\\nDetailed classification report:'\n",
    "    print classification_report(y_test, y_pred, digits=5)\n",
    "    return rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curves(models, scores):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = plt.gca()\n",
    "    fs=14\n",
    "    \n",
    "    for rf, label in zip(models,scores):\n",
    "        probs = rf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        text = ' (area = {:.3f})'.format(roc_auc)\n",
    "        ax.plot(fpr, tpr, label=label + text)\n",
    "    \n",
    "    ax.set_xlabel('False positive rate', fontsize=fs)\n",
    "    ax.set_ylabel('True positive rate', fontsize=fs)\n",
    "    ax.set_title('ROC curve', fontsize=fs)\n",
    "    ax.set_xticks(np.arange(0.0, 1.01, 0.2))\n",
    "    ax.set_xticklabels(np.arange(0.0,1.01,0.2), fontsize=fs)\n",
    "    ax.set_yticks(np.arange(0.0, 1.01, 0.2))\n",
    "    ax.set_yticklabels(np.arange(0.0,1.01,0.2), fontsize=fs)\n",
    "    ax.set_ylim((0,1.01))\n",
    "    ax.set_xlim((-0.01, 1.0))\n",
    "    plt.legend(fontsize=fs, bbox_to_anchor=(1.05,0.5), loc='center left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "Use scikit-learn modules to run a grid search for a random forest model to find the best parameters. Three cases are run, optimized on different scores: accuracy, precision and recall. Confusion matrices are displayed as well as a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop('savings', axis=1)\n",
    "y = data.loc[:,'savings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_data_sets(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [20, 40], \n",
    "              'max_depth': [6, None],\n",
    "              'max_features' : ['sqrt', 40],\n",
    "              'min_samples_split' : [1, 2]}\n",
    "\n",
    "parameters = {'n_estimators': [10], 'max_depth': [6], 'max_features' : ['sqrt'], 'min_samples_split' : [1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_a = run_grid_search(RandomForestClassifier(), parameters,\n",
    "                       score='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_p = run_grid_search(RandomForestClassifier(), parameters,\n",
    "                       score='precision', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_r = run_grid_search(RandomForestClassifier(), parameters,\n",
    "                       score='recall', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc_curves([rf_a, rf_p, rf_r],['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Use one-hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ohe_data.drop('savings', axis=1)\n",
    "y = ohe_data.loc[:,'savings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_data_sets(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {'penalty': ['l1', 'l2'], 'C': [1.0, 0.5, 0.1, 0.05, 0.01]}\n",
    "\n",
    "parameters = {'penalty': ['l1', 'l2'], 'C': [0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_a = run_grid_search(LogisticRegression(), score='accuracy', parameters,\n",
    "                       verbose=1, print_grid_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_p = run_grid_search(LogisticRegression(), score='precision', parameters,\n",
    "                       verbose=1, print_grid_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_r = run_grid_search(LogisticRegression(), score='recall', parameters,\n",
    "                       verbose=1, print_grid_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc_curves([lr_a, lr_p, lr_r],['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Some Feature Engineering\n",
    "Start with the column \"detailed_household_family_stat\" and convert the classes that have no savings greater than 50K into one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_column(column):\n",
    "    dhfs = data[column][data['savings'] == 1].unique()\n",
    "    dhfs.sort()\n",
    "    print 'unique values for svngs = 1', dhfs\n",
    "    dhfs_all = data[column].unique()\n",
    "    dhfs_all.sort()\n",
    "    print 'unique values for all vals ', dhfs_all\n",
    "    \n",
    "    diff = set(dhfs_all).difference(set(dhfs))\n",
    "    print ' the differences are........', diff\n",
    "    if diff is None:\n",
    "        print '\\n diff is empty'\n",
    "        return data[column]\n",
    "    \n",
    "    print ' len(diff)', len(diff)\n",
    "    \n",
    "    val = max(diff) + 1\n",
    "    print ' mapping values to:', val\n",
    "    return data[column].map(lambda x : val if x in diff else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data['detailed_household_family_stat'][data['savings']==0].value_counts().shape\n",
    "data['detailed_household_family_stat'] = update_column('detailed_household_family_stat')\n",
    "print data['detailed_household_family_stat'][data['savings']==0].value_counts().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data['family_members_under_18'][data['savings']==0].value_counts().shape\n",
    "data['family_members_under_18'] = update_column('family_members_under_18')\n",
    "print data['family_members_under_18'][data['savings']==0].value_counts().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 40, max_depth=None, max_features=40, min_samples_split=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print 'confusion matrix:'\n",
    "print_confusion_matrix(y_test, y_pred)\n",
    "print '\\nDetailed classification report:'\n",
    "print classification_report(y_test, y_pred, digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_vis = pca.fit_transform(X)\n",
    "print 'shape(data_vis):', data_vis.shape\n",
    "print data_vis[:4,:]\n",
    "print data_vis[-4:,:]\n",
    "print 'pca.components_.shape:', pca.components_.shape\n",
    "print 'pca.explained_variance_ratio_:', pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'y==0 : ', (y==0).sum()\n",
    "print 'y==1 : ', (y==1).sum()\n",
    "print 'y==0 + y==1:', (y==0).sum() + (y==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the original data\n",
    "# Plot the two classes\n",
    "\n",
    "def scatter_plot(X, y):\n",
    "    data_vis = pca.fit_transform(X)\n",
    "    yeq0 = data_vis[ (y==0) ]\n",
    "    yeq1 = data_vis[ (y==1) ]\n",
    "    palette = sns.color_palette()\n",
    "    almost_black = '#262626'\n",
    "    fig=plt.figure(figsize=(9,9));\n",
    "    ax = fig.gca();\n",
    "    ax.scatter(yeq0[:, 0], yeq0[:, 1], label=\"Savings < 50K\", alpha=0.3, facecolor=palette[0], \n",
    "               linewidth=0.15, edgecolor=almost_black);\n",
    "    ax.scatter(yeq1[:, 0], yeq1[:, 1], label=\"Savings > 50K\", alpha=0.3, facecolor=palette[2], \n",
    "               linewidth=0.15, edgecolor=almost_black);\n",
    "    ax.legend(fontsize=16, loc='lower left', bbox_to_anchor=(1,0.8));\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = scatter_plot(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Class Imblance: Under and Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the new dataset using under-sampling method\n",
    "verbose = False\n",
    "\n",
    "# 'Random under-sampling'\n",
    "US = RandomUnderSampler()\n",
    "usx, usy = US.fit_sample(X, y)\n",
    "ax = scatter_plot(usx, usy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
